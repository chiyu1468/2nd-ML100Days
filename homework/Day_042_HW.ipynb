{"cells":[{"source":"# Change directory to VSCode workspace root so that relative path loads work correctly. Turn this addition off with the DataScience.changeDirOnImportExport setting\nimport os\ntry:\n\tos.chdir(os.path.join(os.getcwd(), '..'))\n\tprint(os.getcwd())\nexcept:\n\tpass\n","cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","metadata":{},"source":[" ## [範例重點]\n"," 了解機器學習建模的步驟、資料型態以及評估結果等流程"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from sklearn import datasets, metrics\n","\n","# 如果是分類問題，請使用 DecisionTreeClassifier，若為回歸問題，請使用 DecisionTreeRegressor\n","from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz\n","from sklearn.model_selection import train_test_split\n","import graphviz\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ## 建立模型四步驟\n","\n"," 在 Scikit-learn 中，建立一個機器學習的模型其實非常簡單，流程大略是以下四個步驟\n","\n"," 1. 讀進資料，並檢查資料的 shape (有多少 samples (rows), 多少 features (columns)，label 的型態是什麼？)\n","     - 讀取資料的方法：\n","         - **使用 pandas 讀取 .csv 檔：**pd.read_csv\n","         - **使用 numpy 讀取 .txt 檔：**np.loadtxt\n","         - **使用 Scikit-learn 內建的資料集：**sklearn.datasets.load_xxx\n","     - **檢查資料數量：**data.shape (data should be np.array or dataframe)\n"," 2. 將資料切為訓練 (train) / 測試 (test)\n","     - train_test_split(data)\n"," 3. 建立模型，將資料 fit 進模型開始訓練\n","     - clf = DecisionTreeClassifier()\n","     - clf.fit(x_train, y_train)\n"," 4. 將測試資料 (features) 放進訓練好的模型中，得到 prediction，與測試資料的 label (y_test) 做評估\n","     - clf.predict(x_test)\n","     - accuracy_score(y_test, y_pred)\n","     - f1_score(y_test, y_pred)"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# 讀取鳶尾花資料集\n","iris = datasets.load_iris()\n","\n","# 切分訓練集/測試集\n","x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.25, random_state=4)\n","\n","# 建立模型\n","clf = DecisionTreeClassifier()\n","\n","# 訓練模型\n","clf.fit(x_train, y_train)\n","\n","# 預測測試集\n","y_pred = clf.predict(x_test)\n","\n",""]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Acuuracy:  0.9736842105263158\n"}],"source":["acc = metrics.accuracy_score(y_test, y_pred)\n","print(\"Acuuracy: \", acc)\n","\n",""]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Features:  ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\nFeature importance:  [0.         0.01796599 0.52229134 0.45974266]\n"}],"source":["print(\"Features: \", iris.feature_names)\n","print(\"Feature importance: \", clf.feature_importances_)\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ## 作業\n","\n"," 目前你應該已經要很清楚資料集中，資料的型態是什麼樣子囉！包含特徵 (features) 與標籤 (labels)。\n"," 因此要記得未來不管什麼專案，必須要把資料清理成相同的格式，才能送進模型訓練。\n"," 今天的作業開始踏入決策樹這個非常重要的模型，請務必確保你理解模型中每個超參數的意思，\n"," 並試著調整看看，對最終預測結果的影響為何\n"," 1. 試著調整 DecisionTreeClassifier(...) 中的參數，並觀察是否會改變結果？\n"," 2. 改用其他資料集 (boston, wine)，並與回歸模型的結果進行比較"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Acuuracy:  0.8888888888888888\n"}],"source":["wine = datasets.load_wine()\n","# boston = datasets.load_boston()\n","# breast_cancer = datasets.load_breast_cancer()\n","\n","# 切分訓練集/測試集\n","x_train, x_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.25, random_state=4)\n","\n","# 建立模型\n","clf = DecisionTreeClassifier(max_depth=3, min_impurity_decrease=1e-7)\n","\n","# 訓練模型\n","clf.fit(x_train, y_train)\n","\n","# 預測測試集\n","y_pred = clf.predict(x_test)\n","\n","acc = metrics.accuracy_score(y_test, y_pred)\n","print(\"Acuuracy: \", acc)\n",""]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Features:  ['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\nFeature importance:  [0.         0.         0.         0.         0.04609792 0.01931722\n 0.0449625  0.         0.         0.35268695 0.         0.13022458\n 0.40671082]\n"}],"source":["print(\"Features: \", wine.feature_names)\n","print(\"Feature importance: \", clf.feature_importances_)\n",""]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/plain":"'Day_042_dtree_render.png'"},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["dot_data = export_graphviz(clf, out_file=None, feature_names=wine.feature_names,  \n","    class_names=wine.target_names, filled=True, rounded=True, special_characters=True)\n","graph = graphviz.Source(dot_data)\n","graph.render('Day_042_dtree_render',view=True, format='png')"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":""}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}